{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation using Sidekick\n",
    "\n",
    "Use the Deployment API of the Peltarion platform via Sidekick to get predictions on samples and evaluate the performance of the deployed model in more detail.\n",
    "\n",
    "**Note**: This notebook requires installation of Sidekick. To install the package within the notebook, run the following code:\n",
    "\n",
    "import sys !{sys.executable} -m pip install git+https://github.com/Peltarion/sidekick#egg=sidekick\n",
    "\n",
    "For more information about Sidekick, see: https://github.com/Peltarion/sidekick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import itertools\n",
    "import os\n",
    "import operator\n",
    "import resource\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import sidekick\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed dataset\n",
    "dataset_path = './fruits-360/Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client to deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the URL and token\n",
    "deployment_url = 'https://...'\n",
    "deployment_token = '...'\n",
    "\n",
    "client = sidekick.Deployment(\n",
    "    url=deployment_url,\n",
    "    token=deployment_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_score(pred):\n",
    "    max_key = 'None'\n",
    "    max_score = 0\n",
    "    dict = pred['fruit_class'].items()\n",
    "    for key,score in dict:\n",
    "        if score >= max_score:        \n",
    "            max_key = key\n",
    "            max_score = score\n",
    "    return (max_key, max_score)\n",
    "\n",
    "\n",
    "def get_image(path):\n",
    "    im = Image.open(path)\n",
    "    new_im = im.copy()\n",
    "    new_im.format = 'jpeg'\n",
    "    im.close()\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress bars for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test dataframe - ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_rel_path = glob(dataset_path + '/*/*.jpg') + glob(dataset_path + '/*/*.png')\n",
    "print(\"Images found: \", len(images_rel_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'image': images_rel_path})\n",
    "df['fruit_class'] = df['image'].progress_apply(lambda path: os.path.basename(os.path.dirname(path)))\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single  predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path_list = iter(list(df['image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = next(im_path_list)\n",
    "im = Image.open(im_path)\n",
    "display(im)\n",
    "pred = client.predict(image=im)\n",
    "print(get_max_score(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.sample(n=10)\n",
    "for i, row in rows.iterrows():\n",
    "    im = Image.open(row['image'])    \n",
    "    display(im)\n",
    "    pred = client.predict(image=im)\n",
    "    print('Ground truth: {}\\nPrediction: {}'.format(row['fruit_class'], get_max_score(pred)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worst misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the image URLs to a new column.\n",
    "Create a new column that contains the actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = df.copy()\n",
    "eval_df['image_url'] = eval_df['image']\n",
    "eval_df['image'] = eval_df['image'].progress_apply(lambda path: get_image(path))\n",
    "predictions = client.predict_lazy(eval_df.to_dict('record'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the predictions and store these in a new column.\n",
    "\n",
    "**Note**: This may take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=len(eval_df))\n",
    "preds = []\n",
    "for p in predictions:\n",
    "    preds.append(p)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['pred'] = [p['fruit_class'] for p in preds]\n",
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new columns that contain the name of highest scoring class and the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = eval_df['pred']\n",
    "max_keys = []\n",
    "max_scores = []\n",
    "\n",
    "for i in dicts:\n",
    "    max_val = max(i.items(), key=lambda k: k[1])     \n",
    "    max_keys.append(max_val[0])\n",
    "    max_scores.append(max_val[1])\n",
    "eval_df['pred_class'] = max_keys\n",
    "eval_df['pred_score'] = max_scores\n",
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top-hundred samples with the highest prediction score and incorrect label, i.e., the worst misclasified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df = eval_df.loc[eval_df['fruit_class'] != eval_df['pred_class']]\n",
    "wrong_df = wrong_df.sort_values(by=['pred_score'], ascending=False)\n",
    "first_rows = wrong_df.head(100)\n",
    "for i, row in first_rows.iterrows():\n",
    "    display(row['image'])\n",
    "    print('Ground truth: {}, Prediction: {}, Score: {}'.format(row['fruit_class'], row['pred_class'], row['pred_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of miclassified Apple Granny Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_granny_smith = wrong_df.loc[df['fruit_class'] == 'Apple Granny Smith']\n",
    "print('Number of misclasified Apple Granny Smith: {}'.format(df_granny_smith.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View all misclassified samples in class Apple Granny Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_granny_smith.iterrows():\n",
    "    im = row['image']\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=eval_df['pred_class'], y_true=eval_df['fruit_class']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (wrong_df.shape[0] / df.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
