{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x119b59dd8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from disk. Replace /YOUR_PATH/model.h5\n",
    "model_path = 'MnistClassifier.h5'\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# Load example image from disk\n",
    "img_path='img_5363.png'\n",
    "img = np.array(Image.open(img_path))\n",
    "\n",
    "%matplotlib inline\n",
    "showme = Image.open(img_path, 'r')\n",
    "imshow(np.asarray(showme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BGR channel ordering. This is not to be done if using cv2 to load the image.\n",
    "img = img[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image\n",
    "img = img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outer shape 1, for batch size\n",
    "img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_part to have 2 dimensions, but got array with shape (1, 28, 28, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f0b0d6cacfa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1486\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    180\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_part to have 2 dimensions, but got array with shape (1, 28, 28, 3)"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "model.predict(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
